import streamlit as st

# Структура данных для хранения информации о статьях
articles = [
    {
        "title": "Как я строил проект DWH",
        "description": "Пройденный путь от посмотреть чему лучше учиться, до построения большого сроекта с DWH",
        "full_text": """
        <div style="text-align: justify; text-indent: 30px; width: 50%;">
            Появилась идея собрать данные с сайта поиска работы и узнать какие технологии сейчас наиболее 
            востребованы на рынке. Чтобы изучать то, что точно требуется и отложить то, что никому не нужно.   
            Начал с того, что решил написать, что мне от нового проекта, собственно, нужно.  <br>
            <br>
            - Сбор данных с сайта. <br> 
            - Выбор нужной информации по вакансиям.  <br>
            - Сохранение в базу чтобы можно было как-то всё отсортировать и подсчитать.<br>  
            <br>
                По ходу пришла идея что это можно собирать постоянно. Тогда в каждый момент времени у меня будет достаточно 
            актуальная информация по требованиям рынка вакансий. А чтобы аналитика была точнее собирать стоит не только 
            по своему городу и своей профессии. Но и в целом по стране и по всему набору IT направлений.  
            В итоге получилось, что мне нужен код, который будет сам периодически запускаться, собирать большой объем 
            данных и отправлять их в базу. Я буду брать данные из базы и сортировкой с фильтрами получать нужную 
            аналитику.  <br>
            <br>
            В первом приближении план готов, погнали.  <br>
            <br>
            Нашел документацию по api hh. Понял, что нужен токен на 
            который надо подавать заявку. Хотелось первые результаты получить быстрее и стал дальше читать документацию 
            чтобы понять можно ли что-то сделать без него. К тому же в интернете видел статью, где использовали api hh 
            без токена. Оказалось, что поиск вакансий и правда можно проводить без него. Там даже много информации по 
            вакансии получить можно и никаких проверок при этом не проходит. Собрал парсер на requests, получил первые 
            данные, загрузил в базу. Дело пошло, но по ограничениям api я смог собрать только 2000 вакансий с одного 
            запроса.  <br>
            <br>
            Не проблема. Накидал макет словаря с параметрами запроса, включая регион, текст поискового запроса и 
            специализацию. Составил массивы с кодами регионов, поисковыми запросами и кодами специализаций. Через циклы 
            всё это перебрал и получил большой массив параметров для поиска. Запустил и стал смотреть. В итоге у меня 
            получилось 4500 наборов параметров для поиска. Каждый набор обрабатывался 5-10 секунд, результаты сохранялись 
            в память, а потом это всё должно было уйти в базу. После 3000 работа сильно замедлилась, а к 3500 у меня 
            окончательно закончилась оперативная память и парсер встал. Не проблема. Разбил наборы параметров на пакеты 
            по 1000 штук и начал обрабатывать их по очереди. Всё сработало, но хранить это всё в одной таблице показалось 
            опасным. Рано или поздно я наберу столько данных что работать она будет очень медленно, а места занимать 
            очень много. Решил изучить как вообще хранятся большие базы данных и как там справляются с моими проблемами. 
            Понял, что ближе всего к тому, что мне нужно это Data Warehouse (DWH). Для моего проекта основная идея 
            состоит в разделении базы данных на слои и разбиении данных на основную таблицу и максимальное количество 
            словарей.  <br>
            <br>
            Первый слой это Stage. Туда данные попадают без обработки. В том виде в котором они получены с сайта или 
            любого другого источника данных. Это дает больше свободы в процессе парсинга. Не надо загромождать код 
            обработкой данных и парсинг в целом идет быстрее. <br>
            <br>
            Второй слой Core предназначен для длительного хранения данных. Тут основная цель уменьшить вес базы. 
            Происходит это за счет создания словарей. Допустим у нас есть 
            столбец отвечающий за необходимый опыт. На HH.ru в нем всего четыре возможных значения: «Нет опыта», 
            «от 1 года до 3 лет», «от 3 до 6 лет», «более 6 лет». Эти значения заносятся в виде текста, самого тяжелого 
            для базы типа данных. И этих строчек должно быть по одной на каждую запись которых может быть и сто тысяч и 
            миллионы в зависимости от базы. А мы переносим сами значения в словарь где будет всего четыре строчки которым 
            соответствуют уникальные id. А в основной таблице заполняем столбец опыта этими самыми id. Вместо текстовых 
            значений у нас остается всего четыре цифры для которых можно выбрать очень легкий тип данных. И так со всем 
            что можно убрать в словари.  <br>
            <br>
            Третий слой Data Mart. Нужен для того чтобы было легко получать данные из базы. Там создаются представления и 
            материализации таблиц. Один раз пишем код на sql и можем запрашивать нужные данные практически в один щелчок. 
            Это же помогает настроить безопасность. Можно создать пользователей с доступом на чтение только тех элементов 
            Data Mart на содержание которых у них есть доступ.  <br>
            <br>
            <img src='https://raw.githubusercontent.com/kulakov544/site_vizitka/main/file/dwh_1.png' style='width: 70%; height: auto;'><br>
            <br>
            Разные слои создаются как разные слои в базе PostgreSQL. Сразу создал все таблицы и связи между ними. 
            Трансформация между слоями Stage и Core будет происходить в самой базе с помощью процедур. Останется только 
            запустить их из кода в конце парсинга. К этой схеме дорисовал свой парсер чтобы было наглядно видно что 
            предстоит реализовывать. <br> 
            <img src='https://raw.githubusercontent.com/kulakov544/site_vizitka/main/file/dwh_2.png' style='width: 100%; height: auto;'><br>
            <br>
            Реализовал схему в коде. Провел первый сбор данных, начал писать процедуры для переноса данных в Core и 
            понял, что главного не хватает. Нет скилов которые идут отдельно в конце большинства вакансий. Снова чтение 
            документации по api. Выяснилось, что получить подробную информацию без токена всё же нельзя и придется 
            получать токен.  <br>
            <br>
            Пока шло одобрение моей заявки на токен добавил оркестрацию через prefect, теперь мой код будет сам 
            запускаться ночью и работать пока не получит все данные. Добавил в базу таблицу с календарем. <br>
            <br>
            Получив одобрение на токен в очередной раз понял как сложно разбираться в некоторых документациях. 
            У hh.ru документация красиво оформлена, вроде всё есть. Но большая часть информации где то в базе откуда её 
            надо доставать get запросами. А та что есть иногда написана для тех кому надо вспомнить как это делается, а 
            не разобраться с нуля. Я далеко не сразу понял что там есть два токена. Один для пользователя, второй для 
            приложения. И оба нужно получать очередным запросом в котором надо отправлять выданные коды. Причем там есть ещё 
            и промежуточный код который надо получить нажав на кнопку на странице по самостоятельно сгенерированному адресу. 
            И этот дополнительный код надо суметь получить, так как передается он так же в ответе от сервера.
            Естественно в первый раз я получил токен для пользователя и долго пытался понять почему мне не дают делать 
            много запросов к базе. Пришлось написать отдельный код для получения нужного токена. К счастью он дается на неограниченное время.<br>
            <br>
            После получения токена пришлось переписать основной код парсера. Теперь я сначала получал id вакансий тем же
             способом что и раньше. А потом запрашивал подробную информацию по этим вакансиям. Так как для каждой вакансии указано 
            несколько скилов пришлось сразу выделить для них отдельную таблицу в Stag. Иначе у меня бы размножились 
            вакансии в основной таблице с и этим пришлось бы что-то делать.  <br>
            <br>
            Попробовал сделать что-нибудь со скоростью работы кода, но не смог. Вся проблема в том, что теперь я на 
            каждую вакансию делаю один отдельный запрос. Их много, на них требуется время, а api вообще ограничивает 
            запросы в секунду. В результате десятки тысяч вакансий обрабатываются почти весь день. <br>
            <img src='https://raw.githubusercontent.com/kulakov544/site_vizitka/main/file/dwh_5.png' style='width: 100%; height: auto;'><br>
            <br>
            Написал процедуры для укладывания данных в Core. Там обошлось почти без проблем. Разве что даже в моей 
            маленькой базе пришлось думать об оптимизации в некоторых местах.  <br>
            <br>
            И на этом этапе чисто случайно заметил, что некоторые зарплаты указаны в долларах. Знал я об этом и раньше, 
            но совершенно не обращал внимания. В будущем зарплаты в валюте не дадут нормально отсортировать данные, 
            а значит это надо исправлять. Нужно получить курсы валют и перевести зарплаты в рубли. Причем делать это надо 
            динамически при переходе в Data Mart внутри представления. Тогда я получу актуальные зарплаты в рублях, 
            а не те, которые были на момент сохранения данных. Ну, а с всё просто. Забрать актуальные курсы с сайта ЦБ, 
            внести в отдельную таблицу. В коде представления умножать на последний курс нужной валюты.  <br>
            <br>
            И вот оно получилось. У меня есть база с нужными данными и с ней можно работать. Сразу дополнил схему тем как 
            хочу с ней работать.  <br>
            <img src='https://raw.githubusercontent.com/kulakov544/site_vizitka/main/file/dwh_3.png' style='width: 100%; height: auto;'><br>
            <br>
            А за одно нарисовал схему базы данных.
            <br>
            <img src='https://raw.githubusercontent.com/kulakov544/site_vizitka/main/file/dwh_6.png' style='width: 100%; height: auto;'><br>
            <br>
            <br>
            <img src='https://raw.githubusercontent.com/kulakov544/site_vizitka/main/file/dwh_7.png' style='width: 100%; height: auto;'><br>
            <br>
            <br>
            <img src='https://raw.githubusercontent.com/kulakov544/site_vizitka/main/file/dwh_8.png' style='width: 100%; height: auto;'><br>
            <br>
            В блоке визуализации я работаю только с теми данными, которые попадают в Data Mart. Только нужные данные и 
            только то что я хочу видеть. Дашборды построены в двух вариантах. Yandex DataLens подключенный к базе. 
             И PowerBI с загруженными из базы данными. Для PowerBI так же поднят PowerBI Report Server c MS SQL Server 
             на домашнем сервере.<br> 
            <br>
            <img src='https://raw.githubusercontent.com/kulakov544/site_vizitka/main/file/dwh_4.png' style='width: 100%; height: auto;'><br>
            <br>
        </div>
        """
    },
    {
        "title": "Инфраструктура проектов: железо и сети.",
        "description": "Описание процесса создания моего первого проекта.",
        "full_text":
        """<div style="text-align: justify; text-indent: 30px; width: 50%;">
            Когда я начал делать свои проекты передо мной возникла неожиданная проблема. Мне их оказалось негде 
            запустить. Если код отработал и выключился то можно это делать на своем рабочем компьютере. Но у меня 
            есть сайт, телеграмм-бот, базы данных. И это всё должно работать постоянно и иметь доступ в интернет. 
            А что будет если в квартире свет отрубят или просто надо будет выключить постоянно работающий компьютер... 
            Пока это не коммерческие проекты ничего страшного, но всё равно не хочется беспокоиться о подобном.<br>
            Что у меня есть сейчас:<br>
            <img src='https://raw.githubusercontent.com/kulakov544/site_vizitka/main/file/Инфраструктура1.png' style='width: 100%; height: auto;'><br>
            <br>
            Компьютер настроенный для работы. Обсидиан для заметок. Яндекс диск для хранения файлов. Там 
            автоматически синхронизируются все мои важные файлы и если что то случился с компьютером я их не потеряю. 
            GitHab для хранения удаленных репозиториев проектов.<br>
            <br>
            В эту схему хочется добавить VPS сервер для легких проектов которые должны просто постоянно крутиться и 
            иметь связь с интернетом. И домашний сервер на который можно нагрузить тяжелые тестовые и учебные базы 
            данных. В итоге получилось вот так:<br>
            <br>
            <img src='https://raw.githubusercontent.com/kulakov544/site_vizitka/main/file/Инфраструктура.png' style='width: 100%; height: auto;'><br>
            <br>
            Первое что я добавил это VPS. Самая дешевая комплектация на Beget. На нем будут мелкие проекты, 
            а так же он будет координировать все мои машины. На нем поднята VPN сеть через WireGuard, к которой будут 
            подключены все мои машины. На нем же установлен Nginx, для того чтобы давать доступ к моим программам из 
            интернета по доменному имени. Ну и Docker для удобного запуска проектов.<br>
            <br>
            Домашний сервер поставить дороже, но в отличии от VPS потратится нужно только один раз. Подойдет любой 
            компьютер достаточно мощный для будущих проектов. А при необходимости мощности можно будет добавить. Для 
            начала можно поискать БУ офисный компьютер. Его просто подключаем к VPN сети и устанавливаем на него всё 
            необходимое. Приоритет отводим проектам с большими требованиями к объему жесткого диска. Так же на него 
            можно поставить Windows и запускать то чем неудобно управлять из командной строки. Установка Windows 
            имеет ещё одно преимущество. Возможность подключаться через RDP. В этом случае на вашем компьютере 
            открывается окно с рабочим столом домашнего сервера.<br>
            <br>
            В итоге получается система удобная для работы. Достаточно отказоустойчива в сравнении с тем что было 
            раньше. Позволяющая разворачивать практически любые свои проекты. Остается отметить только то что главным 
            тут получается VPS сервер и если бы за него не надо было столько платить каждый месяц, я бы просто взял 
            конфигурацию по мощнее и не отвлекался на домашний сервер. Так было бы гораздо удобнее и надежнее.
        </div>
        """
    },
    {
        "title": "Кое-что из санкционного - поднимаем свой сервер Power BI",
        "description": "Описание процесса создания моего первого проекта.",
        "full_text": """
        <div style="text-align: justify; text-indent: 30px; width: 50%;">
            адо было столько платить каждый месяц, я бы просто взял 
            конфигурацию по мощнее и не отвлекался на домашний сервер. Так было бы гораздо удобнее и надежнее. 
            </div>"""
    },
]


def show_blog():
    st.header("Мой блог")
    for index, article in enumerate(articles):
        st.subheader(article["title"])
        st.write(article["description"])
        button_key = f"read_more_{index}"
        if st.button("Читать дальше", key=button_key):
            st.session_state.page = "article"
            st.session_state.selected_article = article
            st.rerun()


def show_full_article():
    if 'selected_article' in st.session_state:
        article = st.session_state.selected_article
        st.sidebar.title("Меню")
        menu_options = ["Обо мне", "Мой блог", "Проекты", "Навыки", "Образование", "Опыт работы"]

        for option in menu_options:
            if st.sidebar.button(option, key=option):
                st.session_state.page = option
                st.session_state.selected_article = None
                st.rerun()

        st.title(article["title"])
        st.markdown(article["full_text"], unsafe_allow_html=True)
        if st.button("Назад", key="back_button"):
            st.session_state.page = "Мой блог"
            del st.session_state.selected_article
            st.rerun()
